{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'task2_ideas.ipynb', 'non-augmented', 'handwriting-recognition', 'Bachelor-s-thesis', 'ws-siamese', 'Thesis', 'htr_self_supervised', 'Paper-ext', 'lost_cities']\n",
      "/Users/lisakoopmans/Documents/GitHub/handwriting-recognition/src_semi\n",
      "/Users/lisakoopmans/Documents/GitHub/handwriting-recognition/src_semi\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "print(os.listdir('../../'))\n",
    "print(os.getcwd())\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../src/network\")\n",
    "sys.path.append(\"../src\")\n",
    "print(os.getcwd())\n",
    "import torch\n",
    "import glob\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import string\n",
    "import os, sys\n",
    "import torch.utils.data as D\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "# from src.network.gen_model.gen_model import GenModel_FC\n",
    "# from torchvision.transforms import v2\n",
    "from random import choices\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sia_oov(file):\n",
    "    \"\"\"Get data paths and labels (with max_word_len) of images in folder.\"\"\"\n",
    "    unique_words = {}\n",
    "    lens = {}\n",
    "\n",
    "    with open(file) as data_file:\n",
    "        lines = data_file.read().splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line_split = line.split(' ')\n",
    "                \n",
    "        if len(line_split) > 2:\n",
    "            img_path, gt_label = line_split[0], ''.join(line_split[1:])\n",
    "        else:\n",
    "            img_path, gt_label = line_split[0], line_split[1]\n",
    "        \n",
    "        if gt_label in unique_words.keys():\n",
    "            unique_words[gt_label] += 1\n",
    "        else:\n",
    "            unique_words[gt_label] = 1\n",
    "            \n",
    "        \n",
    "        if len(gt_label) in lens.keys():\n",
    "                lens[len(gt_label)] += 1\n",
    "        else:\n",
    "            lens[len(gt_label)] = 1\n",
    "                \n",
    "            \n",
    "    print(f\"number of unique words: {len(unique_words.keys())}\")\n",
    "\n",
    "    lens = dict(sorted(lens.items()))\n",
    "\n",
    "    print(f\"Number of words per word length\")\n",
    "    for key in lens.keys():\n",
    "        print(key, lens[key])\n",
    "\n",
    "    return unique_words, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_htr(folder):\n",
    "    \"\"\"Get data paths and labels (with max_word_len) of images in folder.\"\"\"\n",
    "    partitions = ['train', 'valid', 'test']\n",
    "    dataset = []\n",
    "    wid_dict = {}\n",
    "    unique_words = {}\n",
    "    wids = []\n",
    "\n",
    "    for partition in partitions:\n",
    "        lens = {}\n",
    "        print(partition)\n",
    "\n",
    "        text_file = os.path.join(folder, f\"ground_truth_{partition}_filtered.txt\")\n",
    "\n",
    "        with open(text_file, encoding='utf-8') as data_file:\n",
    "            lines = data_file.read().splitlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line_split = line.split(' ')\n",
    "            wid = None\n",
    "\n",
    "            if not set(string.digits).isdisjoint(set(line_split[-1])):\n",
    "                wid = line_split[-1]\n",
    "                line_split = line_split[:-1]\n",
    "                \n",
    "            if len(line_split) > 2:\n",
    "                img_path, gt_label = line_split[0], ''.join(line_split[1:])\n",
    "            else:\n",
    "                img_path, gt_label = line_split[0], line_split[1]\n",
    "\n",
    "            if gt_label in unique_words.keys():\n",
    "                unique_words[gt_label] += 1\n",
    "            else:\n",
    "                unique_words[gt_label] = 1\n",
    "\n",
    "            if len(gt_label) in lens.keys():\n",
    "                lens[len(gt_label)] += 1\n",
    "            else:\n",
    "                lens[len(gt_label)] = 1\n",
    "            \n",
    "            img_path = img_path.replace(\"/\", \"\\\\\")\n",
    "\n",
    "            if wid is not None:\n",
    "                wids.append(wid)\n",
    "                if wid not in wid_dict.keys():\n",
    "                    wid_dict[wid] = []\n",
    "                \n",
    "                wid_dict[wid].append((img_path, gt_label))\n",
    "\n",
    "            # img_path = os.path.join(folder, partition, img_path)\n",
    "            dataset.append((img_path, gt_label, wid))\n",
    "    print(f\"number of samples: {len(dataset)}\")\n",
    "    print(f\"number of wids: {len(wid_dict.keys())}\")\n",
    "\n",
    "    lens = dict(sorted(lens.items()))\n",
    "\n",
    "    print(f\"Number of words per word length\")\n",
    "    for key in lens.keys():\n",
    "        print(key, lens[key])\n",
    "\n",
    "    return dataset, wid_dict, wids, unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get unique words siamese data sets\n",
    "\n",
    "* All oov words\n",
    "* All unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_oov_folder = \"../../ws-siamese/data/iam/words/supervised_htr/\"\n",
    "\n",
    "sia_train_file = sia_oov_folder + \"ground_truth_train_filtered.txt\"\n",
    "sia_oov_valid_file = sia_oov_folder + \"ground_truth_valid_oov_filtered.txt\"\n",
    "sia_oov_test_file = sia_oov_folder + \"ground_truth_test_oov_filtered.txt\"\n",
    "\n",
    "sia_valid_file = sia_oov_folder + \"ground_truth_valid_filtered.txt\"\n",
    "sia_test_file = sia_oov_folder + \"ground_truth_test_filtered.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words: 3161\n",
      "Number of words per word length\n",
      "2 6415\n",
      "3 7706\n",
      "4 5655\n",
      "5 3798\n",
      "6 2706\n",
      "7 2440\n",
      "number of unique words: 224\n",
      "Number of words per word length\n",
      "2 160\n",
      "3 357\n",
      "4 380\n",
      "5 260\n",
      "6 250\n",
      "7 277\n",
      "number of unique words: 220\n",
      "Number of words per word length\n",
      "2 4\n",
      "3 241\n",
      "4 404\n",
      "5 184\n",
      "6 231\n",
      "7 129\n",
      "number of unique words: 1386\n",
      "Number of words per word length\n",
      "2 1533\n",
      "3 1774\n",
      "4 1306\n",
      "5 963\n",
      "6 662\n",
      "7 570\n",
      "number of unique words: 1545\n",
      "Number of words per word length\n",
      "2 1870\n",
      "3 2296\n",
      "4 1739\n",
      "5 1135\n",
      "6 780\n",
      "7 723\n",
      "dict_keys(['new', 'too', 'loud', 'back', 'writing', 'special', 'cannot', 'wonder', 'stage', 'At', 'man', 'best', 'Duke', 'express', 'years', 'goes', 'Council', 'against', 'ister', 'bundle', 'force', 'sewers', 'mass', 'among', 'Mauro', 'show', 'Richard', 'left', 'avert', 'minimum', 'each', 'odds', 'foreign', 'passed', 'Congo', 'Senator', 'Russia', 'Macleod', 'Tories', 'nothing', 'likely', 'three', 'want', 'social', 'Nuclear', 'capable', 'cinema', 'young', 'women', 'moved', 'mother', 'if', 'she', 'doubt', 'If', 'became', 'Holland', 'pitch', 'rise', 'failing', 'numbers', 'Hayward', 'garden', 'factor', 'dingy', 'such', 'hopes', 'accents', 'worn', 'Towards', 'Glasgow', 'Hood', 'firmly', 'able', 'General', 'obtains', 'Marc', 'spot', 'plot', 'aide', 'amount', 'bedlam', 'boiled', 'muffled', 'Down', 'kissed', 'mad', 'lunch', 'listen', 'brandy', 'ban', 'learn', 'cars', 'order', 'items', 'cured', 'shelter', 'Budget', 'bossed', 'farmers', 'Danish', 'BAFFLED', 'meat', 'Khouang', 'figure', 'gallon', 'Troy', 'Steel', 'Delaney', 'meant', 'Apart', 'outcome', 'dinner', 'bureau', 'dabs', 'Chief', 'reads', 'dress', 'Martyrs', 'patron', 'ous', 'flew', 'Scott', 'rubbing', 'pianist', 'drawers', 'areas', 'worked', 'Staff', 'drawn', 'bits', 'replies', 'binding', 'Spain', 'expose', 'porter', 'lessly', 'father', 'Forty', 'tries', 'Scots', 'damned', 'yes', 'ference', 'exit', 'Bills', 'coats', 'halfway', 'kit', 'step', 'tape', 'Poland', 'feudal', 'delayed', 'Byrne', 'lated', 'Rupert', 'wrinkle', 'drop', 'rallied', 'fuller', 'GO', 'Try', 'Anyway', 'married', 'Dowd', 'miners', 'Swede', 'plates', 'sham', 'placed', 'freedom', 'granted', 'shining', 'sleep', 'offence', 'cocked', 'lake', 'Ethel', 'Huw', 'sadly', 'fairer', 'Little', 'Turton', 'flying', 'Turkish', 'hardest', 'reckon', 'ride', 'anvils', 'Gaunt', 'Joy', 'Charles', 'grand', 'Harlech', 'deep', 'yawl', 'Fill', 'tribe', 'funds', 'wisely', 'animal', 'poorest', 'grave', 'spades', 'NUF', 'Ward', 'lashed', 'Modena', 'Roman', 'waving', 'Pliny', 'restore', 'silk', 'Say', 'martyrs', 'novel', 'Britons', 'puzzled', 'stiffly', 'Boxer', 'animals', 'chance', 'tonic'])\n",
      "dict_keys(['AM', 'add', 'times', 'weekly', 'now', 'Nazi', 'oil', 'birds', 'angle', 'Peel', 'chapter', 'Lord', 'adding', 'Office', 'Heath', 'Common', 'down', 'plan', 'blind', 'Berlin', 'upon', 'happen', 'find', 'rain', 'dealt', 'answer', 'might', 'due', 'urgency', 'during', 'need', 'THE', 'aim', 'easier', 'imposed', 'economy', 'say', 'full', 'meet', 'benches', 'For', 'series', 'Collins', 'queue', 'HERE', 'built', 'Frankie', 'play', 'failed', 'forlorn', 'seen', 'scenes', 'Ferris', 'within', 'rushed', 'ignored', 'pleads', 'green', 'machine', 'possess', 'wears', 'takes', 'comes', 'Hudson', 'arrives', 'Dennis', 'basis', 'weak', 'sweetly', 'Mason', 'clay', 'relieve', 'intense', 'lover', 'motives', 'queer', 'memory', 'facing', 'raucous', 'cough', 'amusing', 'La', 'noses', 'presume', 'strange', 'mean', 'farmer', 'income', 'beach', 'thin', 'clubbed', 'local', 'become', 'ten', 'orders', 'accept', 'Premier', 'Plain', 'sale', 'massed', 'designs', 'mats', 'PARIS', 'yielded', 'motive', 'crowded', 'circles', 'pretty', 'line', 'crooks', 'rob', 'dc', 'unite', 'butt', 'kind', 'develop', 'defend', 'comedy', 'boxes', 'cramped', 'scheme', 'halving', 'expense', 'hotel', 'rity', 'Three', 'Powers', 'Cold', 'Vita', 'raises', 'Jeff', 'drive', 'cries', 'theory', 'eat', 'request', 'Neither', 'wish', 'Zealand', 'havens', 'tatted', 'backed', 'tapped', 'Carlton', 'monk', 'tiger', 'pound', 'Nights', 'toe', 'thud', 'Edie', 'novice', 'allies', 'grunted', 'rigged', 'coffee', 'sacks', 'Thetan', 'bus', 'Greeks', 'Station', 'teach', 'eagle', 'Wills', 'palazzo', 'Pepys', 'laced', 'HEAVEN', 'EARTH', 'violent', 'Chateau', 'manors', 'tires', 'illness', 'diverse', 'sombre', 'Johnny', 'ridge', 'Good', 'stored', 'handled', 'dignity', 'treat', 'Brave', 'Hiding', 'Kagawa', 'Thee', 'gods', 'pushed', 'caraway', 'ries', 'sulphur', 'Guest', 'pool', 'Dante', 'tastes', 'examine', 'Young', 'horn', 'fishing', 'Using', 'Woman', 'Enter', 'fided', 'focused', 'arts', 'fifteen', 'roads', 'measure', 'alive', 'fragile', 'scarf', 'periods', 'Hope', 'packed', 'letting', 'mix', 'destiny', 'Forrest', 'allows'])\n",
      "total number of oov words: 444\n",
      "total number of train words: 3161\n",
      "total number of words: 4419\n",
      "AM\n",
      "La\n",
      "dc\n",
      "406 647 6808 1386\n",
      "total number of oov words: 850\n",
      "total number of train words: 3161\n",
      "477 799 8543 1545\n",
      "total number of oov words: 1258\n",
      "total number of train words: 3161\n"
     ]
    }
   ],
   "source": [
    "sia_train_words, sia_train_lens = read_sia_oov(sia_train_file)\n",
    "sia_oov_valid_words, sia_oov_valid_lens = read_sia_oov(sia_oov_valid_file)\n",
    "sia_oov_test_words, sia_oov_test_lens = read_sia_oov(sia_oov_test_file)\n",
    "\n",
    "sia_valid_words, sia_valid_lens = read_sia_oov(sia_valid_file)\n",
    "sia_test_words, sia_test_lens = read_sia_oov(sia_test_file)\n",
    "\n",
    "print(sia_oov_valid_words.keys())\n",
    "print(sia_oov_test_words.keys())\n",
    "\n",
    "sia_oov_words = set(list(sia_oov_valid_words.keys()) + list(sia_oov_test_words.keys()))\n",
    "sia_words = set(list(sia_oov_valid_words.keys()) + list(sia_oov_test_words.keys()) + list(sia_train_words.keys()) + list(sia_valid_words.keys()) + list(sia_test_words.keys()))\n",
    "print(\"total number of oov words:\", len(sia_oov_words))\n",
    "print(\"total number of train words:\", len(sia_train_words.keys()))\n",
    "print(\"total number of words:\", len(sia_words))\n",
    "\n",
    "for words in sia_oov_test_words.keys():\n",
    "    if len(words) == 2:\n",
    "        print(words)\n",
    "\n",
    "count = 0\n",
    "sample_count = 0\n",
    "for word in sia_valid_words.keys():\n",
    "    if word not in set(list(sia_train_words.keys())):\n",
    "        count += 1\n",
    "        sample_count += sia_valid_words[word]\n",
    "        sia_oov_words.add(word)\n",
    "print(count, sample_count, sum(sia_valid_words.values()), len(sia_valid_words.keys()))\n",
    "\n",
    "print(\"total number of oov words:\", len(sia_oov_words))\n",
    "print(\"total number of train words:\", len(sia_train_words.keys()))\n",
    "\n",
    "count = 0\n",
    "sample_count = 0\n",
    "for word in sia_test_words.keys():\n",
    "    if word not in set(list(sia_train_words.keys())):\n",
    "        count += 1\n",
    "        sample_count += sia_test_words[word]\n",
    "        sia_oov_words.add(word)\n",
    "print(count, sample_count, sum(sia_test_words.values()), len(sia_test_words.keys()))\n",
    "\n",
    "print(\"total number of oov words:\", len(sia_oov_words))\n",
    "print(\"total number of train words:\", len(sia_train_words.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all samples self-HTR dataset\n",
    "\n",
    "* Get all samples, wids, gt_labels\n",
    "* Get all unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "valid\n",
      "test\n",
      "number of samples: 31556\n",
      "number of wids: 499\n",
      "Number of words per word length\n",
      "2 1028\n",
      "3 1288\n",
      "4 984\n",
      "5 663\n",
      "6 495\n",
      "7 434\n",
      "number of uniqe words: 4442\n"
     ]
    }
   ],
   "source": [
    "htr_folder = \"../data/iam/words/\"\n",
    "# htr_train_data, htr_train_wids, htr_train_words = read_htr(htr_folder + \"ground_truth_train_filtered.txt\")\n",
    "# htr_valid_data, htr_valid_wids, htr_valid_words = read_htr(htr_folder + \"ground_truth_valid_filtered.txt\")\n",
    "# htr_test_data, htr_test_wids, htr_test_words = read_htr(htr_folder + \"ground_truth_test_filtered.txt\")\n",
    "\n",
    "htr_dataset, htr_wids_dict, htr_wids, htr_words = read_htr(htr_folder)\n",
    "print(\"number of uniqe words:\", len(list(htr_words.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of oov words: 1772\n",
      "Number of oov samples in self-HTR of which labels are not in dataset super-HTR: 2302\n",
      "Number of samples in HTR of which words are in oov super-HTR: 2356\n",
      "Number of samples in self-HTR of which super-HTR has seen the word: 26898\n",
      "total: 31556\n"
     ]
    }
   ],
   "source": [
    "htr_oov_words = []\n",
    "\n",
    "for word in list(htr_words.keys()):\n",
    "    if word not in sia_words:\n",
    "        htr_oov_words.append(word)\n",
    "\n",
    "print(\"Number of oov words:\", len(htr_oov_words))\n",
    "\n",
    "htr_oov_samples = []\n",
    "htr_wids_oov_samples = []\n",
    "\n",
    "for tup in htr_dataset:\n",
    "    if tup[1] in htr_oov_words:\n",
    "        htr_oov_samples.append(tup)\n",
    "        htr_wids_oov_samples.append(tup[2])\n",
    "\n",
    "print(\"Number of oov samples in self-HTR of which labels are not in dataset super-HTR:\", len(htr_oov_samples))\n",
    "\n",
    "full_oov_samples = []\n",
    "full_wids_oov_samples = []\n",
    "\n",
    "for tup in htr_dataset:\n",
    "    if tup[1] in sia_oov_words:\n",
    "        full_oov_samples.append(tup)\n",
    "        full_wids_oov_samples.append(tup[2])\n",
    "\n",
    "print(\"Number of samples in HTR of which words are in oov super-HTR:\", len(full_oov_samples))\n",
    "\n",
    "htr_iv_samples = []\n",
    "htr_wids_iv_samples = []\n",
    "\n",
    "for tup in htr_dataset:\n",
    "    if tup not in htr_oov_samples:\n",
    "        if tup not in full_oov_samples:\n",
    "            htr_iv_samples.append(tup)\n",
    "            htr_wids_iv_samples.append(tup[2])\n",
    "\n",
    "print(\"Number of samples in self-HTR of which super-HTR has seen the word:\", len(htr_iv_samples))\n",
    "print(\"total:\", len(htr_iv_samples) + len(full_oov_samples) + len(htr_oov_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../data/iam/words/semisup/', exist_ok=True)\n",
    "def write_to_txt(file, data):\n",
    "    with open(file, 'w') as f:\n",
    "        for tup in data:\n",
    "            f.write(f\"{tup[0]} {tup[1]} {tup[2]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split oov super-htr in self-htr 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178 1178\n",
      "('..\\\\data\\\\iam\\\\words\\\\valid\\\\c02\\\\c02-017\\\\c02-017-00-04.png', 'novel', '139') ('..\\\\data\\\\iam\\\\words\\\\train\\\\a02\\\\a02-124\\\\a02-124-04-04.png', 'End', '037')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_valid, X_test, = train_test_split(full_oov_samples, test_size=0.5, random_state=42, shuffle=True)\n",
    "print(len(X_valid), len(X_test))\n",
    "print(X_valid[0], X_test[0])\n",
    "\n",
    "write_to_txt(\"../data/iam/words/semisup/ground_truth_valid_oov_super-in-self_filtered.txt\", X_valid)\n",
    "write_to_txt(\"../data/iam/words/semisup/ground_truth_test_oov_super-in-self_filtered.txt\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80/10/10 for oov not in self-htr and not in super-htr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 231 1841\n",
      "('..\\\\data\\\\iam\\\\words\\\\train\\\\r03\\\\r03-110\\\\r03-110-06-05.png', 'flats', '668') ('..\\\\data\\\\iam\\\\words\\\\train\\\\f07\\\\f07-084b\\\\f07-084b-08-06.png', 'garlic', '291') ('..\\\\data\\\\iam\\\\words\\\\valid\\\\m01\\\\m01-115\\\\m01-115-04-05.png', 'finding', '515')\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, = train_test_split(htr_oov_samples, test_size=0.2, random_state=42, shuffle=True)\n",
    "X_valid, X_test, = train_test_split(X_test, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "print(len(X_valid), len(X_test), len(X_train))\n",
    "print(X_valid[0], X_test[0], X_train[0])\n",
    "\n",
    "write_to_txt(\"../data/iam/words/semisup/ground_truth_train_oov_super-self_filtered.txt\", X_train)\n",
    "write_to_txt(\"../data/iam/words/semisup/ground_truth_valid_oov_super-self_filtered.txt\", X_valid)\n",
    "write_to_txt(\"../data/iam/words/semisup/ground_truth_test_oov_super-self_filtered.txt\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70/15/15 for iv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4035 4035 18828\n",
      "('..\\\\data\\\\iam\\\\words\\\\train\\\\b01\\\\b01-038\\\\b01-038-06-04.png', 'gain', '089') ('..\\\\data\\\\iam\\\\words\\\\valid\\\\a02\\\\a02-057\\\\a02-057-03-05.png', 'day', '030') ('..\\\\data\\\\iam\\\\words\\\\train\\\\n02\\\\n02-049\\\\n02-049-03-00.png', 'look', '561')\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, = train_test_split(htr_iv_samples, test_size=0.3, random_state=42, shuffle=True)\n",
    "X_valid, X_test, = train_test_split(X_test, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "print(len(X_valid), len(X_test), len(X_train))\n",
    "print(X_valid[0], X_test[0], X_train[0])\n",
    "\n",
    "write_to_txt(\"../data/iam/words/semisup/ground_truth_train_iv_super-self_filtered.txt\", X_train)\n",
    "write_to_txt(\"../data/iam/words/semisup/ground_truth_valid_iv_super-self_filtered.txt\", X_valid)\n",
    "write_to_txt(\"../data/iam/words/semisup/ground_truth_test_iv_super-self_filtered.txt\", X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
