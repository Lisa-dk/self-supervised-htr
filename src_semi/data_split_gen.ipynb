{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'handwriting-recognition', 'htr_self_supervised', 'manuscript-dating-sn', 'mnist_pipeline', 'Paper-ext', 'self-supervised-mnist', 'Thesis', 'ws-siamese']\n",
      "c:\\Users\\Lisa\\Documents\\GitHub\\handwriting-recognition\\src_semi\n",
      "c:\\Users\\Lisa\\Documents\\GitHub\\handwriting-recognition\\src_semi\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "print(os.listdir('../../'))\n",
    "print(os.getcwd())\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../src/network\")\n",
    "sys.path.append(\"../src\")\n",
    "print(os.getcwd())\n",
    "import torch\n",
    "import glob\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from src.data.tokenizer import Tokenizer\n",
    "import string\n",
    "import os, sys\n",
    "import torch.utils.data as D\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from src.network.gen_model.gen_model import GenModel_FC\n",
    "from torchvision.transforms import v2\n",
    "from random import choices\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from kornia_moons.viz import *\n",
    "from src.data.data_loader import RIMES_data\n",
    "from src.data.reader import read_rimes\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from src.network.losses import Loss\n",
    "from src.network.sia_model.model import SiameseNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_oov_folder = \"../../ws-siamese/data/iam/words/supervised_htr/\"\n",
    "\n",
    "sia_train_file = sia_oov_folder + \"ground_truth_train_filtered.txt\"\n",
    "sia_oov_valid_file = sia_oov_folder + \"ground_truth_valid_oov_filtered.txt\"\n",
    "sia_oov_test_file = sia_oov_folder + \"ground_truth_test_oov_filtered.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sia_oov(file):\n",
    "    \"\"\"Get data paths and labels (with max_word_len) of images in folder.\"\"\"\n",
    "    unique_words = {}\n",
    "    lens = {}\n",
    "\n",
    "    with open(file) as data_file:\n",
    "        lines = data_file.read().splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line_split = line.split(' ')\n",
    "                \n",
    "        if len(line_split) > 2:\n",
    "            img_path, gt_label = line_split[0], ''.join(line_split[1:])\n",
    "        else:\n",
    "            img_path, gt_label = line_split[0], line_split[1]\n",
    "        \n",
    "        if gt_label in unique_words.keys():\n",
    "            unique_words[gt_label] += 1\n",
    "        else:\n",
    "            unique_words[gt_label] = 1\n",
    "            \n",
    "        \n",
    "        if len(gt_label) in lens.keys():\n",
    "                lens[len(gt_label)] += 1\n",
    "        else:\n",
    "            lens[len(gt_label)] = 1\n",
    "                \n",
    "            \n",
    "    print(f\"number of unique words: {len(unique_words.keys())}\")\n",
    "\n",
    "    lens = dict(sorted(lens.items()))\n",
    "\n",
    "    print(f\"Number of words per word length\")\n",
    "    for key in lens.keys():\n",
    "        print(key, lens[key])\n",
    "\n",
    "    return unique_words, lens\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_htr(text_file):\n",
    "    \"\"\"Get data paths and labels (with max_word_len) of images in folder.\"\"\"\n",
    "    dataset = []\n",
    "    wid_dict = {}\n",
    "    lens = {}\n",
    "    unique_words = {}\n",
    "\n",
    "    with open(text_file, encoding='utf-8') as data_file:\n",
    "        lines = data_file.read().splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line_split = line.split(' ')\n",
    "        wid = None\n",
    "\n",
    "        if not set(string.digits).isdisjoint(set(line_split[-1])):\n",
    "            wid = line_split[-1]\n",
    "            line_split = line_split[:-1]\n",
    "                \n",
    "        if len(line_split) > 2:\n",
    "            img_path, gt_label = line_split[0], ''.join(line_split[1:])\n",
    "        else:\n",
    "            img_path, gt_label = line_split[0], line_split[1]\n",
    "                \n",
    "        if gt_label in unique_words.keys():\n",
    "            unique_words[gt_label] += 1\n",
    "        else:\n",
    "            unique_words[gt_label] = 1\n",
    "\n",
    "        if len(gt_label) in lens.keys():\n",
    "            lens[len(gt_label)] += 1\n",
    "        else:\n",
    "            lens[len(gt_label)] = 1\n",
    "            \n",
    "            img_path = img_path.replace(\"/\", \"\\\\\")\n",
    "\n",
    "            if wid is not None:\n",
    "                if wid not in wid_dict.keys():\n",
    "                    wid_dict[wid] = []\n",
    "                \n",
    "                wid_dict[wid].append((img_path, gt_label))\n",
    "\n",
    "            # img_path = os.path.join(folder, partition, img_path)\n",
    "            dataset.append((img_path, gt_label, wid))\n",
    "    print(f\"number of words: {len(dataset)}\")\n",
    "    print(f\"number of wids: {len(wid_dict.keys())}\")\n",
    "\n",
    "    lens = dict(sorted(lens.items()))\n",
    "\n",
    "    print(f\"Number of words per word length\")\n",
    "    for key in lens.keys():\n",
    "        print(key, lens[key])\n",
    "\n",
    "    return dataset, wid_dict, unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words: 3161\n",
      "Number of words per word length\n",
      "2 6415\n",
      "3 7706\n",
      "4 5655\n",
      "5 3798\n",
      "6 2706\n",
      "7 2440\n",
      "number of unique words: 224\n",
      "Number of words per word length\n",
      "2 160\n",
      "3 357\n",
      "4 380\n",
      "5 260\n",
      "6 250\n",
      "7 277\n",
      "number of unique words: 220\n",
      "Number of words per word length\n",
      "2 4\n",
      "3 241\n",
      "4 404\n",
      "5 184\n",
      "6 231\n",
      "7 129\n",
      "dict_keys(['new', 'too', 'loud', 'back', 'writing', 'special', 'cannot', 'wonder', 'stage', 'At', 'man', 'best', 'Duke', 'express', 'years', 'goes', 'Council', 'against', 'ister', 'bundle', 'force', 'sewers', 'mass', 'among', 'Mauro', 'show', 'Richard', 'left', 'avert', 'minimum', 'each', 'odds', 'foreign', 'passed', 'Congo', 'Senator', 'Russia', 'Macleod', 'Tories', 'nothing', 'likely', 'three', 'want', 'social', 'Nuclear', 'capable', 'cinema', 'young', 'women', 'moved', 'mother', 'if', 'she', 'doubt', 'If', 'became', 'Holland', 'pitch', 'rise', 'failing', 'numbers', 'Hayward', 'garden', 'factor', 'dingy', 'such', 'hopes', 'accents', 'worn', 'Towards', 'Glasgow', 'Hood', 'firmly', 'able', 'General', 'obtains', 'Marc', 'spot', 'plot', 'aide', 'amount', 'bedlam', 'boiled', 'muffled', 'Down', 'kissed', 'mad', 'lunch', 'listen', 'brandy', 'ban', 'learn', 'cars', 'order', 'items', 'cured', 'shelter', 'Budget', 'bossed', 'farmers', 'Danish', 'BAFFLED', 'meat', 'Khouang', 'figure', 'gallon', 'Troy', 'Steel', 'Delaney', 'meant', 'Apart', 'outcome', 'dinner', 'bureau', 'dabs', 'Chief', 'reads', 'dress', 'Martyrs', 'patron', 'ous', 'flew', 'Scott', 'rubbing', 'pianist', 'drawers', 'areas', 'worked', 'Staff', 'drawn', 'bits', 'replies', 'binding', 'Spain', 'expose', 'porter', 'lessly', 'father', 'Forty', 'tries', 'Scots', 'damned', 'yes', 'ference', 'exit', 'Bills', 'coats', 'halfway', 'kit', 'step', 'tape', 'Poland', 'feudal', 'delayed', 'Byrne', 'lated', 'Rupert', 'wrinkle', 'drop', 'rallied', 'fuller', 'GO', 'Try', 'Anyway', 'married', 'Dowd', 'miners', 'Swede', 'plates', 'sham', 'placed', 'freedom', 'granted', 'shining', 'sleep', 'offence', 'cocked', 'lake', 'Ethel', 'Huw', 'sadly', 'fairer', 'Little', 'Turton', 'flying', 'Turkish', 'hardest', 'reckon', 'ride', 'anvils', 'Gaunt', 'Joy', 'Charles', 'grand', 'Harlech', 'deep', 'yawl', 'Fill', 'tribe', 'funds', 'wisely', 'animal', 'poorest', 'grave', 'spades', 'NUF', 'Ward', 'lashed', 'Modena', 'Roman', 'waving', 'Pliny', 'restore', 'silk', 'Say', 'martyrs', 'novel', 'Britons', 'puzzled', 'stiffly', 'Boxer', 'animals', 'chance', 'tonic'])\n",
      "dict_keys(['AM', 'add', 'times', 'weekly', 'now', 'Nazi', 'oil', 'birds', 'angle', 'Peel', 'chapter', 'Lord', 'adding', 'Office', 'Heath', 'Common', 'down', 'plan', 'blind', 'Berlin', 'upon', 'happen', 'find', 'rain', 'dealt', 'answer', 'might', 'due', 'urgency', 'during', 'need', 'THE', 'aim', 'easier', 'imposed', 'economy', 'say', 'full', 'meet', 'benches', 'For', 'series', 'Collins', 'queue', 'HERE', 'built', 'Frankie', 'play', 'failed', 'forlorn', 'seen', 'scenes', 'Ferris', 'within', 'rushed', 'ignored', 'pleads', 'green', 'machine', 'possess', 'wears', 'takes', 'comes', 'Hudson', 'arrives', 'Dennis', 'basis', 'weak', 'sweetly', 'Mason', 'clay', 'relieve', 'intense', 'lover', 'motives', 'queer', 'memory', 'facing', 'raucous', 'cough', 'amusing', 'La', 'noses', 'presume', 'strange', 'mean', 'farmer', 'income', 'beach', 'thin', 'clubbed', 'local', 'become', 'ten', 'orders', 'accept', 'Premier', 'Plain', 'sale', 'massed', 'designs', 'mats', 'PARIS', 'yielded', 'motive', 'crowded', 'circles', 'pretty', 'line', 'crooks', 'rob', 'dc', 'unite', 'butt', 'kind', 'develop', 'defend', 'comedy', 'boxes', 'cramped', 'scheme', 'halving', 'expense', 'hotel', 'rity', 'Three', 'Powers', 'Cold', 'Vita', 'raises', 'Jeff', 'drive', 'cries', 'theory', 'eat', 'request', 'Neither', 'wish', 'Zealand', 'havens', 'tatted', 'backed', 'tapped', 'Carlton', 'monk', 'tiger', 'pound', 'Nights', 'toe', 'thud', 'Edie', 'novice', 'allies', 'grunted', 'rigged', 'coffee', 'sacks', 'Thetan', 'bus', 'Greeks', 'Station', 'teach', 'eagle', 'Wills', 'palazzo', 'Pepys', 'laced', 'HEAVEN', 'EARTH', 'violent', 'Chateau', 'manors', 'tires', 'illness', 'diverse', 'sombre', 'Johnny', 'ridge', 'Good', 'stored', 'handled', 'dignity', 'treat', 'Brave', 'Hiding', 'Kagawa', 'Thee', 'gods', 'pushed', 'caraway', 'ries', 'sulphur', 'Guest', 'pool', 'Dante', 'tastes', 'examine', 'Young', 'horn', 'fishing', 'Using', 'Woman', 'Enter', 'fided', 'focused', 'arts', 'fifteen', 'roads', 'measure', 'alive', 'fragile', 'scarf', 'periods', 'Hope', 'packed', 'letting', 'mix', 'destiny', 'Forrest', 'allows'])\n",
      "total number of oov words: 444\n",
      "3161\n"
     ]
    }
   ],
   "source": [
    "sia_train_words, sia_train_lens = read_sia_oov(sia_train_file)\n",
    "sia_oov_valid_words, sia_oov_valid_lens = read_sia_oov(sia_oov_valid_file)\n",
    "sia_oov_test_words, sia_oov_test_lens = read_sia_oov(sia_oov_test_file)\n",
    "\n",
    "print(sia_oov_valid_words.keys())\n",
    "print(sia_oov_test_words.keys())\n",
    "\n",
    "sia_oov_words = set(list(sia_oov_valid_words.keys()) + list(sia_oov_test_words.keys()))\n",
    "sia_words = set(list(sia_oov_valid_words.keys()) + list(sia_oov_test_words.keys()) + list(sia_train_words.keys()))\n",
    "print(\"total number of oov words:\", len(sia_oov_words))\n",
    "print(len(sia_train_words.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 6\n",
      "number of wids: 1\n",
      "Number of words per word length\n",
      "2 4301\n",
      "3 5306\n",
      "4 4147\n",
      "5 2841\n",
      "6 2128\n",
      "7 1829\n",
      "number of words: 6\n",
      "number of wids: 2\n",
      "Number of words per word length\n",
      "2 1281\n",
      "3 1605\n",
      "4 1264\n",
      "5 808\n",
      "6 637\n",
      "7 517\n",
      "number of words: 6\n",
      "number of wids: 2\n",
      "Number of words per word length\n",
      "2 1028\n",
      "3 1288\n",
      "4 984\n",
      "5 663\n",
      "6 495\n",
      "7 434\n",
      "3539\n"
     ]
    }
   ],
   "source": [
    "htr_folder = \"../data/iam/words/\"\n",
    "htr_train_data, htr_train_wids, htr_train_words = read_htr(htr_folder + \"ground_truth_train_filtered.txt\")\n",
    "htr_valid_data, htr_valid_wids, htr_valid_words = read_htr(htr_folder + \"ground_truth_valid_filtered.txt\")\n",
    "htr_test_data, htr_test_wids, htr_test_words = read_htr(htr_folder + \"ground_truth_test_filtered.txt\")\n",
    "\n",
    "print(len(htr_train_words.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501\n",
      "549\n",
      "419\n",
      "276\n"
     ]
    }
   ],
   "source": [
    "htr_oov_train = []\n",
    "\n",
    "for word in list(htr_train_words.keys()):\n",
    "    if word not in sia_oov_words:\n",
    "        if word not in list(sia_train_words.keys()):\n",
    "            htr_oov_train.append(word)\n",
    "\n",
    "print(len(set(htr_oov_train)))\n",
    "\n",
    "htr_oov_valid = []\n",
    "\n",
    "for word in list(htr_valid_words.keys()):\n",
    "    if word not in sia_oov_words:\n",
    "        if word not in list(sia_train_words.keys()):\n",
    "            htr_oov_valid.append(word)\n",
    "\n",
    "print(len(set(htr_oov_valid)))\n",
    "\n",
    "htr_oov_test = []\n",
    "\n",
    "for word in list(htr_test_words.keys()):\n",
    "    if word not in sia_oov_words:\n",
    "        if word not in list(sia_train_words.keys()):\n",
    "            htr_oov_test.append(word)\n",
    "\n",
    "print(len(set(htr_oov_test)))\n",
    "\n",
    "htr_oov_test_oov = []\n",
    "\n",
    "for word in set(htr_oov_test):\n",
    "    if word not in htr_oov_train:\n",
    "        if word not in set(htr_oov_train):\n",
    "            htr_oov_test_oov.append(word)\n",
    "\n",
    "print(len(set(htr_oov_test_oov)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
