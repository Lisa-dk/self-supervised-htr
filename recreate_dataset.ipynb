{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '.RData', '.Rhistory', '220[1].jpg', 'Aangepaste Office-sjablonen', 'ADS_programming_report_template.pdf', \"Assassin's Creed Origins\", 'BioWare', 'desktop.ini', 'Downloads', 'FeedbackHub', 'GitHub', 'Graphics', 'Mijn afbeeldingen', 'Mijn muziek', \"Mijn video's\", 'My Games', 'Need for Speed Heat', 'Python Notebooks', 'Python Scripts', 'Scripts', 'Untitled.ipynb', 'Visual Studio 2019']\n",
      "c:\\Users\\Lisa\\Documents\\GitHub\\handwriting-recognition\n",
      "c:\\Users\\Lisa\\Documents\\GitHub\\handwriting-recognition\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(\"./src/network\")\n",
    "sys.path.append(\"./src\")\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from random import choices\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "import torch.utils.data as D\n",
    "\n",
    "from src.network.gen_model.gen_model import GenModel_FC\n",
    "from src.data.tokenizer import Tokenizer\n",
    "from src.data.data_loader import IAM_data\n",
    "from src.data.reader import read_iam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(folder, max_word_len, synth):\n",
    "    \"\"\"Get data paths and labels (with max_word_len) of images in folder.\"\"\"\n",
    "\n",
    "    partitions = ['train', 'valid']\n",
    "    dataset = {}\n",
    "    wid_dict = {}\n",
    "    for partition in partitions:\n",
    "        lens = {}\n",
    "        print(partition)\n",
    "        dataset[partition] = []\n",
    "        wid_dict[partition] = {}\n",
    "\n",
    "        text_file = os.path.join(folder, f\"ground_truth_{partition}_filtered.txt\")\n",
    "\n",
    "        with open(text_file, encoding='utf-8') as data_file:\n",
    "            lines = data_file.read().splitlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line_split = line.split(' ')\n",
    "            wid = None\n",
    "\n",
    "            if not set(string.digits).isdisjoint(set(line_split[-1])):\n",
    "                wid = line_split[-1]\n",
    "                line_split = line_split[:-1]\n",
    "                \n",
    "            if len(line_split) > 2:\n",
    "                img_path, gt_label = line_split[0], ''.join(line_split[1:])\n",
    "            else:\n",
    "                img_path, gt_label = line_split[0], line_split[1]\n",
    "                \n",
    "            if len(gt_label) > max_word_len:\n",
    "                continue\n",
    "\n",
    "            if len(gt_label) in lens.keys():\n",
    "                lens[len(gt_label)] += 1\n",
    "            else:\n",
    "                lens[len(gt_label)] = 1\n",
    "            \n",
    "            img_path = img_path.replace(\"/\", \"\\\\\")\n",
    "\n",
    "            if wid is not None:\n",
    "                if wid not in wid_dict[partition].keys():\n",
    "                    wid_dict[partition][wid] = []\n",
    "                \n",
    "                wid_dict[partition][wid].append((img_path, gt_label))\n",
    "            \n",
    "            if synth:\n",
    "                img_path_split = img_path.split('\\\\')\n",
    "                img_path = os.path.join(img_path_split[0], img_path_split[1], img_path_split[2], img_path_split[3], \"valid_gen\", img_path_split[-1])\n",
    "            else:\n",
    "                img_path = img_path[1:]\n",
    "\n",
    "            # img_path = os.path.join(folder, partition, img_path)\n",
    "            dataset[partition].append((img_path[1:], gt_label, wid))\n",
    "        print(f\"number of words in {partition}: {len(dataset[partition])}\")\n",
    "        print(f\"number of wids in {partition}: {len(wid_dict[partition].keys())}\")\n",
    "    \n",
    "        lens = dict(sorted(lens.items()))\n",
    "\n",
    "        print(f\"Number of words per word length\")\n",
    "        for key in lens.keys():\n",
    "            print(key, lens[key])\n",
    "\n",
    "    # Adding training samples to wid dictionaries for validation and testing sets\n",
    "    for wid in wid_dict['train'].keys():\n",
    "        if wid in wid_dict['valid'].keys():\n",
    "            wid_dict['valid'][wid] += wid_dict['train'][wid]\n",
    "\n",
    "    return dataset['train'], dataset['valid'],  wid_dict['train'], wid_dict['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset_base = string.ascii_lowercase + string.ascii_uppercase\n",
    "max_text_length = 25\n",
    "tokenizer = Tokenizer(chars=charset_base, max_text_length=max_text_length, ctc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_epochs = 2500\n",
    "gen_model = GenModel_FC(tokenizer.maxlen, tokenizer.vocab_size, tokenizer.PAD).cuda()\n",
    "gen_model.load_state_dict(torch.load(f'./src/network/gen_model/gen_model-{gen_epochs}-25ch-corr-half.model')) #load model\n",
    "gen_model.eval()\n",
    "\n",
    "for param in gen_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = (64, 216, 1)\n",
    "num_style_imgs = 25 # num imgs for generator to extract style from\n",
    "batch_size = 48\n",
    "dataset = \"iam_gan\"\n",
    "dataset_path = os.path.join(\"data\", dataset, \"words\")\n",
    "max_word_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "number of words in train: 19744\n",
      "number of wids in train: 500\n",
      "Number of words per word length\n",
      "2 4645\n",
      "3 5617\n",
      "4 4030\n",
      "5 2404\n",
      "6 1608\n",
      "7 1440\n",
      "valid\n",
      "number of words in valid: 4105\n",
      "number of wids in valid: 499\n",
      "Number of words per word length\n",
      "2 954\n",
      "3 1135\n",
      "4 806\n",
      "5 513\n",
      "6 373\n",
      "7 324\n",
      "test\n",
      "number of words in test: 4357\n",
      "number of wids in test: 500\n",
      "Number of words per word length\n",
      "2 978\n",
      "3 1251\n",
      "4 898\n",
      "5 552\n",
      "6 371\n",
      "7 307\n",
      "oov_train\n",
      "number of words in oov_train: 1324\n",
      "number of wids in oov_train: 306\n",
      "Number of words per word length\n",
      "2 13\n",
      "3 76\n",
      "4 257\n",
      "5 334\n",
      "6 358\n",
      "7 286\n",
      "oov_valid\n",
      "number of words in oov_valid: 866\n",
      "number of wids in oov_valid: 329\n",
      "Number of words per word length\n",
      "2 9\n",
      "3 58\n",
      "4 181\n",
      "5 218\n",
      "6 230\n",
      "7 170\n",
      "oov_test\n",
      "number of words in oov_test: 1164\n",
      "number of wids in oov_test: 352\n",
      "Number of words per word length\n",
      "2 12\n",
      "3 63\n",
      "4 223\n",
      "5 293\n",
      "6 320\n",
      "7 253\n"
     ]
    }
   ],
   "source": [
    "data_train, data_valid, data_test, oov_data_train, oov_data_valid, oov_data_test, wid_train, wid_valid, wid_test, oov_wid_train, oov_wid_valid, oov_wid_test = read_iam(dataset_path, 10, synth=False)\n",
    "        \n",
    "oov_data_valid = IAM_data(oov_data_valid, input_size=input_size, tokenizer=tokenizer, num_images=num_style_imgs, wids=oov_wid_valid)\n",
    "oov_valid_loader = torch.utils.data.DataLoader(oov_data_valid, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "data_test = IAM_data(data_test, input_size=input_size, tokenizer=tokenizer, num_images=num_style_imgs, wids=wid_test)\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "oov_data_test = IAM_data(oov_data_test, input_size=input_size, tokenizer=tokenizer, num_images=num_style_imgs, wids=oov_wid_test)\n",
    "oov_test_loader = torch.utils.data.DataLoader(oov_data_test, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "data_train = IAM_data(data_train, input_size=input_size, tokenizer=tokenizer, num_images=num_style_imgs, wids=wid_train)\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=1)\n",
    "\n",
    "oov_data_train = IAM_data(oov_data_train, input_size=input_size, tokenizer=tokenizer, num_images=num_style_imgs, wids=wid_train)\n",
    "oov_train_loader = torch.utils.data.DataLoader(oov_data_train, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=1)\n",
    "\n",
    "data_valid = IAM_data(data_valid, input_size=input_size, tokenizer=tokenizer, num_images=num_style_imgs, wids=wid_valid)\n",
    "valid_loader = torch.utils.data.DataLoader(data_valid, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate a dataset of IAM-HTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in oov_test_loader:\n",
    "    imgs, gen_imgs, gt_labels, wid, img_paths = batch\n",
    "    gt_labels = torch.nn.functional.one_hot(gt_labels.long(), 56).float()\n",
    "\n",
    "    gen_imgs = gen_imgs.to(device).squeeze(2)\n",
    "    gt_labels = gt_labels.to(device)\n",
    "\n",
    "    imgs = imgs.numpy()\n",
    "    imgs = 255. - (((imgs * 0.5) + 0.5) * 255)\n",
    "        \n",
    "    synth_imgs = gen_model(gen_imgs, gt_labels).detach()\n",
    "    synth_imgs = 255. - (((synth_imgs * 0.5) + 0.5) * 255)\n",
    "    synth_imgs = synth_imgs.cpu().numpy()\n",
    "\n",
    "    for idx in range(len(img_paths)):\n",
    "\n",
    "        path_split = img_paths[idx].split('\\\\')\n",
    "        img_name = path_split[-1][:-4]  # Removal of .png \n",
    "        img_name = img_name + '_synth.png'\n",
    "        new_img_path = os.path.join('.', path_split[1], path_split[2], path_split[3], path_split[4], path_split[5], path_split[6], img_name)\n",
    "\n",
    "        # path_split = img_paths[idx].split('.png')[0]\n",
    "        # new_img_path = path_split + '_synth.png'\n",
    "        cv2.imwrite(new_img_path, synth_imgs[idx][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in oov_test_loader:\n",
    "    imgs, gen_imgs, gt_labels, wid, img_paths = batch\n",
    "    gt_labels = torch.nn.functional.one_hot(gt_labels.long(), 56).float()\n",
    "\n",
    "    gen_imgs = gen_imgs.to(device).squeeze(2)\n",
    "    gt_labels = gt_labels.to(device)\n",
    "\n",
    "    imgs = imgs.numpy()\n",
    "    imgs = 255. - (((imgs * 0.5) + 0.5) * 255)\n",
    "\n",
    "        \n",
    "    synth_imgs = gen_model(gen_imgs, gt_labels).detach()\n",
    "    synth_imgs = 255. - (((synth_imgs * 0.5) + 0.5) * 255)\n",
    "    synth_imgs = synth_imgs.cpu().numpy()\n",
    "\n",
    "    for idx in range(len(img_paths)):\n",
    "\n",
    "        path_split = img_paths[idx].split('\\\\')\n",
    "        img_name = path_split[-1][:-4]  # remove .png \n",
    "        img_name = img_name + '_synth.png'\n",
    "        new_img_path = os.path.join('.', path_split[1], path_split[2], path_split[3], path_split[4], path_split[5], path_split[6], img_name)\n",
    "\n",
    "        # path_split = img_paths[idx].split('.png')[0]\n",
    "        # new_img_path = path_split + '_synth.png'\n",
    "        cv2.imwrite(new_img_path, synth_imgs[idx][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate the IAM-GEN validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "number of words in train: 22117\n",
      "number of wids in train: 339\n",
      "Number of words per word length\n",
      "2 4817\n",
      "3 5670\n",
      "4 4367\n",
      "5 2981\n",
      "6 2242\n",
      "7 2040\n",
      "valid\n",
      "number of words in valid: 9178\n",
      "number of wids in valid: 161\n",
      "Number of words per word length\n",
      "2 1860\n",
      "3 2502\n",
      "4 1937\n",
      "5 1261\n",
      "6 867\n",
      "7 751\n"
     ]
    }
   ],
   "source": [
    "data_train, data_valid, wid_train, wid_valid = read_data(dataset_path, 10, synth=False)\n",
    "\n",
    "data_valid = IAM_data(data_valid, input_size=input_size, tokenizer=tokenizer, num_images=num_style_imgs, wids=wid_valid)\n",
    "valid_loader = torch.utils.data.DataLoader(data_valid, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_loader:\n",
    "    imgs, gen_imgs, gt_labels, wid, img_paths = batch\n",
    "    gt_labels = torch.nn.functional.one_hot(gt_labels.long(), 56).float()\n",
    "\n",
    "    gen_imgs = gen_imgs.to(device).squeeze(2)\n",
    "    gt_labels = gt_labels.to(device)\n",
    "\n",
    "    imgs = imgs.numpy()\n",
    "    imgs = 255. - (((imgs * 0.5) + 0.5) * 255)\n",
    "\n",
    "        \n",
    "    synth_imgs = gen_model(gen_imgs, gt_labels).detach()\n",
    "    synth_imgs = 255. - (((synth_imgs * 0.5) + 0.5) * 255)\n",
    "    synth_imgs = synth_imgs.cpu().numpy()\n",
    "\n",
    "    for idx in range(len(img_paths)):\n",
    "        \n",
    "        path_split = img_paths[idx].split('\\\\')\n",
    "        img_name = path_split[-1][:-4] # Removal of .png \n",
    "        new_dir = os.path.join(\"data\", dataset, \"words\", \"valid_gen_2500\")\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "        new_img_path = os.path.join(new_dir, img_name + f'.png')\n",
    "\n",
    "        cv2.imwrite(new_img_path, synth_imgs[idx][0])\n",
    "\n",
    "        # Uncomment to save images in folders per wid\n",
    "        # new_dir = os.path.join(\"data\", dataset, \"words\", \"valid_gen_1750\", wid[idx])\n",
    "        # os.makedirs(new_dir, exist_ok=True)\n",
    "        # new_img_path = os.path.join(new_dir, img_name + f'.png')\n",
    "        # cv2.imwrite(new_img_path, synth_imgs[idx][0])\n",
    "\n",
    "        # if i == 0:\n",
    "        #     new_dir = os.path.join(\"data\", dataset, \"words\", \"valid\", wid[idx])\n",
    "        #     os.makedirs(new_dir, exist_ok=True)\n",
    "        #     new_img_path = os.path.join(new_dir, path_split[-1])\n",
    "        #     cv2.imwrite(new_img_path, imgs[idx][0])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
